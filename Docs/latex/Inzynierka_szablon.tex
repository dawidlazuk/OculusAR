\documentclass[a4paper,11pt,twoside]{report}


% -------------- Kodowanie znaków, język polski -------------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
%\usepackage[T1]{fontenc}
\usepackage[english, polish]{babel}


% ----------------- Przydatne pakiety ----------------------
\usepackage{amsfonts}
\usepackage{mathrsfs} 
\usepackage{amsmath,amsthm,latexsym,xpatch}
\usepackage[dvips]{graphicx,color}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{array}
\usepackage{pstricks}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{xcolor}

% ---------------- Marginesy, akapity, interlinia ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
%\allowdisplaybreaks
\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z wcięciem
\setlength{\parindent}{5mm}

\hyphenation{Syl-ves-tra}
\hyphenation{Syl-ves-ter-a}

%--------------------------- ŻYWA PAGINA ------------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia


\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}



% ---------------- Nagłówki rozdziałów ---------------------

\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- Spis treści ---------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- Spisy tabel i obrazków ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i obrazki razem


% --------------------- Definicje, twierdzenia etc. ---------------


\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                          % Space above
{3ex}%                          % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother

% ----------------------------- POLSKI --------------------------------
\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}

% If in English, comment this and uncomment below:

% ----------------------------- ENGLISH -----------------------------
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}[chapter]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{example}[theorem]{Example}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{remark}[theorem]{Remark}



% ----------------------------- Dowód -----------------------------

\makeatletter
\renewenvironment{proof}[1][\proofname]
{\par
  \vspace{-12pt}% remove the space after the theorem
  \pushQED{\qed}%
  \normalfont
  \topsep0pt \partopsep0pt % no space before
  \trivlist
  \item[\hskip\labelsep
        \sc
    #1\@addpunct{:}]\ignorespaces
}
{%
  \popQED\endtrivlist\@endpefalse
  \addvspace{20pt} % some space after
}

\renewcommand{\qedhere}{\hfill \qedsymbol}
\makeatother





% -------------------------- POCZĄTEK --------------------------


% --------------------- Ustawienia użytkownika ------------------

\newcommand{\tytul}{Gogle AR jako modyfikacja gogli VR}
\renewcommand{\title}{AR googles as modified VR googles}
\renewcommand{\author}{Dawid Łazuk, Piotr Piwowarski}
\newcommand{\album}{268791,000000}
\newcommand{\type}{inżyniers} % magisters, licencjac (Master or Engineer in English)
\newcommand{\supervisor}{dr inż. Krzysztof Kaczmarski}

\lstdefinestyle{sharpc}{language=[Sharp]C, rulecolor=\color{blue!80!black}}

\begin{document}
\sloppy

% \selectlanguage{english} % uncomment this for English 

% ----------------------- Abstrakty -----------------------------

%\selectlanguage{polish}
\begin{abstract}

\begin{center}
\tytul
\end{center}

Streszczam.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Słowa kluczowe:} slowo1, slowo2, ...
\end{abstract}

\null\thispagestyle{empty}\newpage

{\selectlanguage{english}
\begin{abstract}

\begin{center}
\title
\end{center}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Keywords:} keyword1, keyword2, ...
\end{abstract}}


\null\thispagestyle{empty}\newpage

\null \hfill Warszawa, dnia ..................\\

\par\vspace{5cm}

\begin{center}
Oświadczenie % Declaration - for English
\end{center}

\indent Oświadczam, że pracę \type ką pod
tytułem ,,\tytul '', której promotorem jest \supervisor \ wykonałam/em
samodzielnie, co poświadczam własnoręcznym podpisem.
\vspace{2cm}

% English:

%I hereby declare that the thesis entitled ,,\title '', submitted for the \type ~degree, supervised  by \supervisor , is entirely my original work apart from the recognized reference.
%\vspace{2cm}

\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage

\null\thispagestyle{empty}\newpage
% ------------------- 4. Spis treści ---------------------
% \selectlanguage{english} - for English

\tableofcontents
\thispagestyle{empty}
\newpage
% -------------- 5. ZASADNICZA CZĘŚĆ PRACY --------------------
\null\thispagestyle{empty}\newpage
\setcounter{page}{11}
\pagestyle{fancy}


\chapter*{Wstęp} % Intruduction
\markboth{}{Wstęp}
\addcontentsline{toc}{chapter}{Wstęp}

Szybki rozwój techniki spowodował zwiększenie jej obecności w codziennym życiu. Rozwój interfejsów użytkownika i badania nad jego wrażeniami z wykorzystywania różnych systemów spowodowały powstanie technologii takich jak wirtualna i rozszerzona rzeczywistość. Służą one zarówno do rozrywki jak i też do ułatwienia życia i usprawnienia pracy ludziom. Rozwiązania tego typu stają się coraz popularniejsze i można zaobserwować rosnące nimi zainteresowanie w wielu branżach.

Dlatego też, w niniejszej pracy podjęto się zbadania możliwości zastosowania gogli wirtualnej rzeczywistości w aplikacjach wykorzystujacych rzeczywistość rozszerzoną.

\chapter{Wstęp teoretyczny}

W tym rozdziale omówione zostaną zagadnienia wirtualnej oraz rozszerzonej rzeczywistości. Poruszone zostaną problemy, jakie wiążą się z zastosowaniem tych technologii. Przedstawiona zostanie też natura wizji stereoskopowej człowieka w zakresie zjawisk, które będą miały wpływ na przebieg prac i ich rezultat.

\section {Wirtualna i rozszerzona rzeczywistość}

\subsection {Definicje i zastosowanie VR/AR}

\begin{definition}[Wirtualna rzeczywistość]
\textit{Wirtualną rzeczywistością} (ang. VR - Virtual Reality) nazywamy wrażenie przebywania użytkownika w świecie stworzonym wirtualnie. Zwykle dotyczy to obrazu, dźwięku oraz interakcji z otoczeniem, ale podejmowane są próby wykorzystania zapachu i dotyku w aplikacjach VR.
\end{definition}

Obecnie większości ludziom termin VR kojarzy się z goglami zakładanymi na głowę użytkownika, które umożliwiają wyświetlanie sceny 3D w taki sposób, że użytkownik ma wrażenie przebywania w tej scenie. Wbudowane w urządzenie żyroskopy reagują na ruchy głowy odwzorowując je w wyświetlanym obrazie. Gogle często posiadają wbudowane głośniki (Oculus Rift) lub możliwość podpięcia słuchawek (HTC Vive), zapewniając także bodźce akustyczne. Takie rozwiązanie pozwala osiągnąć immersję nieosiągalną dla dotychczasowo wykorzystywanych technologii. Nawet w przypadku telewizji trójwymiarowej.

Wirtualna rzeczywistość jest obecnie stosowana na bardzo szeroką skalę. Wykorzystywana jest w przemyśle rozrywkowym w postaci gier VR czy też filmów 360\textdegree, w których użytkownik może poczuć się jakby znajdował się w samym centrum akcji. Rozwiązania VR mają potencjał w przemyśle turystycznym, umożliwiając wirtualne wycieczki po egzotycznych, często trudno dostępnych miejscach. Agenci nieruchomości mogą umożliwić zainteresowanym klientom obejrzenie mieszkania czy domu jeszcze przed powstaniem budynku. Wystarczy tylko stworzyć odpowiedni model 3D i wyświetlić go w goglach wirtualnej rzeczywistości. Technologia znalazła też zastosowanie w szkoleniu żołnierzy, umożliwiając przeprowadzenie symulacji operacji na wirtualnych poligonach.

\textbf {źródło: http://www.komputerswiat.pl/centrum-wiedzy-konsumenta/gaming/wszystko-o-wirtualnej-rzeczywistosci/co-to-jest-wirtualna-rzeczywistosc.aspx }

\begin{definition}[Rozszerzona rzeczywistość]
\textit{Rozszerzoną rzeczywistością} (ang. AR - Augmented Reality) nazywamy nałożenie na wizję świata rzeczywistego obrazu stworzonego wirtualnie. Użytkownik dzięki temu może na przykład otrzymywać dodatkowe informacje o obiektach, na które patrzy. Systemy AR nie zapewniają interakcji z częścią wirtualną świata.
\end{definition}

Rozwiązania rozszerzonej rzeczywistości wzbogacają zmysły człowieka poprzez przekazanie im dodatkowych informacji. O ile w przypadku wirtualnej rzeczywistości użytkownik, w pewnym sensie, odcinał się od świata rzeczywistego, tak w przypadku rzeczywistości rozszerzonej ten kontakt z rzeczywistością pozostaje. Istnieją samochody oraz myśliwce wyposażone w HUD - przezroczysty ekran wyświetlający informacje takie jak na przykład prędkość, z jaką porusza się samochód, nie zasłaniając tego co za nim się znajduje. Dość znanym zastosowaniem rozszerzonej rzeczywistości jest, wydana w 2016 roku, gra Pokemon GO. Użytkownik przy pomocy swojego smartphone'a spacerując po mieście zbiera pokemony, które wyświetlają się na ekranie telefonu jako obraz nałożony na  świat rzeczywisty, widziany przez kamerę urządzenia.

Przykładami urządzeń AR obecnymi na rynku są Google Glass i Microsoft HoloLens. W odróźnieniu od wirtualnej rzeczywistości, oczy użytkownika nie zostają zasłonięte całkowicie. Urządzenia mają formę okularów, przez które człowiek widzi to co przed nim się znajduje, a obraz wirtualny jest wyświetlany na okularach bądź aplikowany bezpośrednio do oka.

\begin{definition}[Mieszana rzeczywistość]
\textit{Mieszaną rzeczywistością} (ang. MR - Mixed Reality) nazywamy rozszerzenie klasy rozwiązań AR o interakcję użytkownika z wirtualną częścią świata.
\end{definition}

Zagadnienie mieszanej rzeczywistości leży poza zakresem niniejszej pracy. Definicja została zamieszczona w celu rozróżnienia tej klasy rozwiązań od rozwiązań rozszerzonej rzeczywistości.

\subsection { Problemy użytkowe VR/AR }

Wykorzystanie rozwiązań wirtualnej i rozszerzonej rzeczywistości niesie za sobą szereg wyzwań stawianych deweloperom, oraz konsekwencji, których użytkownicy muszą być świadomi.

\begin{description}
\item [Wymogi wydajnościowe] \hfill \\
W celu komfortowego korzystania z rozwiązań wirtualnej rzeczywistości systemy muszą spełniać szereg wymogów wydajnościowych. Organizm ludzki jest przystosowany do częstotliwości zmian obrazu występującej w naturze. Podanie obrazu o częstotliwości dużo niższej będzie powodowało dyskomfort w korzystaniu z aplikacji. Przyjmuje się, że obraz w aplikacjach VR powinien mieć $90 FPS$, a opóźnienie obrazu nie powinno przekraczać $20 ms$.
\item [Wpływ na wzrok] \hfill \\
Nie podlega wątpliwościom, że długotrwała praca przy monitorze szkodzi. Mięśnie gałki ocznej, które w normalnych warunkach są odpowiedzialne za akomodację oka, zastygają w jednej pozycji, ponieważ ekran jest w mniej więcej stałej odległości od oka. Intensywne światło też ma negatywny wpływ na oczy. W przypadku gogli wirtualnej rzeczywistości mamy do czynienia z relatywnie dużym ekranem, zasłaniającym praktycznie całe pole widzenia użytkownika, w odległości rzędu kilkudziesięciu milimetrów od oka. Długotrwała praca oczu w takich warunkach może doprowadzić do pogorszenia wzroku.
\item [Choroba symulacyjna] \hfill \\
Ludzki organizm jest skonstruowany tak, że mózg pobiera informacje dotyczące otoczenia z różnych zmysłów. W normalnych warunkach bodźce z różnych układów odbierane w tym samym czasie odpowiadają sobie. W zagadnieniach wirtualnej rzeczywistości układem zmysłów, powodującym problemy, jest układ oczy - błędnik. W grach lub filmach, w których występuje ruch, oczy użytkownika odbierają bodźce wskazujące na ruch organizmu. Natomiast jeśli użytkownik siedzi na przykład w fotelu, jego błędnik nie wykrywa ruchu. Mózg otrzymuje sprzeczne bodźce co skutkuje objawami podobnymi do choroby lokomocyjnej.
\item [Zbyt wysoka immersja] \hfill \\
Znane są przypadki ludzi umierających przed ekranem monitora po kilkunastu lub kilkudziesięciu godzinach nieustannej gry. Osoby uzależnione w takim stopniu od wirtualnej rozgrywki mają problem z rozróżnianiem świata rzeczywistego od wirtualnego, zapominając nawet o potrzebach fizjologicznych. Wysoka immersja, osiągana przez rozwiązania wirtualnej rzeczywistości, może spotęgować ten problem. Zwłaszcza, że rozrywka VR jest znacznie bardziej wymagająca dla organizmu od tradycyjnej, co już zostało wcześniej opisane. 
Kolejny problem dotyczący immersji jest związany z osobami mającymi problemy kardiologiczne. Realistyczne wrażenia z gier typu horror mogą doprowadzić takie osoby nawet do zawału serca.
\end{description}

\textbf {źródło: http://www.komputerswiat.pl/centrum-wiedzy-konsumenta/gaming/wszystko-o-wirtualnej-rzeczywistosci/niebezpieczenstwa-i-wady-vr.aspx }

\section {Wizja stereoskopowa człowieka}

Człowiek doświadcza widzenia trójwymiarowego dzięki temu, że dwoje jego oczu jest skierowane mniej więcej równolegle w ten sam punkt. Umożliwia to zebranie dwóch obrazów, które różnią się od siebie perspektywą obserwatora. Mózg przetwarzając te obrazy tworzy efekt głębi umożliwiając człowiekowi ocenę odległości przedmiotów, na które patrzy.

\subsection {Odległość między źrenicami}

Badania antropometryczne personelu armii Stanów Zjednoczonych w 2012 roku objęły swoim zakresem odległości między ludzkimi źrenicami. Wyniki tych badań są zawarte w poniższej tabeli. Dane są przedstawione w milimetrach.

\begin{table}[h!]
\centering
\label{my-label}
\begin{tabular}{lcccc}
\textbf{Płeć} & \multicolumn{1}{l}{\textbf{Minimum}} & \multicolumn{1}{l}{\textbf{Maksimum}} & \multicolumn{1}{l}{\textbf{Średnia}} & \multicolumn{1}{l}{\textbf{Mediana}} \\
Kobieta       & 51.0                                 & 74.5                                  & 61.7                                 & 62.0                                 \\
Mężczyzna     & 53.0                                 & 77.0                                  & 64.0                                 & 64.0                                
\end{tabular}
\caption{Odległości między źrenicami}
\end{table}

Z powyższych danych jednoznacznie wynika, że odległość pomiędzy ludzkimi źrenicami zawiera się w przedziale od 51.0 milimetrów do 77.0 milimetrów.

\textbf{(http://www.dtic.mil/docs/citations/ADA634277 - do przypisów)}

\subsection {Akomodacja oka}

\begin{definition}[Akomodacja oka]
\textit{Akomodacją oka} nazywamy zjawisko dostosowania się soczewki oka do odległości od oglądanego przedmiotu, w celu zapewnienia ostrości obrazu.
\end{definition}

U człowieka akomodacja oka jest realizowana poprzez zmianę kształtu soczewki, co skutkuje zmianą jej ogniskowej (zdolności skupiającej).

\begin{definition}[Punkt bliży wzrokowej]
\textit{Punktem bliży wzrokowej} nazywamy minimalną odległość, przy której akomodacja oka wpływa na ostrość obrazu. U człowieka punkt bliży wzrokowej leży w odległości $10 cm$ od oka.
\end{definition}

\begin{definition}[Punkt dali wzrokowej]
\textit{Punktem dali wzrokowej} nazywamy maksymalną odległość, przy której akomodacja oka wpływa na ostrość obrazu. U człowieka punkt dali wzrokowej leży w odległości $6 m$ od oka.
\end{definition}

\subsection {Zbieżność oczu}

\begin{definition}[Zbieżność oczu]
\textit{Zbieżnością oczu} nazywamy zjawisko obrócenia oczu do środka w sytuacji, kiedy obserwowany punkt leży w niewielkiej odległości od obserwatora. W takim przypadku osie widzenia obu oczu przecinają się w jednym punkcie. Jeżeli natomiast obserwowany punkt leży daleko od obserwatora, osie widzenia są równoległe do siebie.
\end{definition}

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{zbieznosc}
\caption[Zbieżność oczu (źródło: http://www.swiatlo.tak.pl/1/index.php/funkcje-wzroku-akomodacja-adaptacja-zbieznosc/)]{Zbieżność oczu}
\end{figure}

\chapter{Cel i założenia projektu} % Cel i założenia projektu

Celem pracy jest stworzenie systemu umożliwiającego przekazanie obrazu z dwóch kamer do gogli wirtualnej rzeczywistości), tworząc tym samym system rozszerzonej rzeczywistości. 

System ma umożliwiać efekt widzenia stereoskopowego, aby użytkownik miał wrażenie normalnej wizji. Efekt ten zostanie osiągnięty poprzez umiejscowienie kamer tak, aby odległość między ich soczewkami odpowiadała odległości pomiędzy źrenicami ludzkich oczu. 

System ma umożliwiać przetwarzanie przechwyconego obrazu zanim trafi on do oczu. Przetwarzanie to może być realizowane na przykład poprzez nałożenie prostych filtrów na obraz, lub też wykorzystania bardziej zaawansowanych algorytmów (na przykład wykrywania twarzy).


\section{Środowisko sprzętowe}


W tym podrozdziale omówione zostaną urządzenia wykorzystane w pracy. Przedstawiona zostanie specyfikacja sprzętu zapewnionego przez Wydział Matematyki i Nauk Informacyjnych Politechniki Warszawskiej. Omówimy także problemy wynikające z wykorzystania danego rodzaju sprzętu oraz propozycje ich rozwiązania.


\subsection{Gogle Oculus Rift}

Oculus Rift to gogle wirtualnej rzeczywistości stworzone przez firmę Oculus VR, będącą obecnie własnością Facebook Inc. Zostały ujawnione światu w 2012 roku na platformie Kickstarter w celu zebrania funduszy na rozwój projektu. 

Oculus Rift wykorzystuje dwa ekrany OLED, po jednym na każde oko. Ekrany te mają rozdzielczość 1080 pikseli na 1200 pikseli. Częstotliwość ich odświeżania wynosi 90 Hz. Pomiędzy ekranami, a oczami użytkownika znajdują się soczewki. Kąty widzenia gogli to 90\textdegree  pionowo i 80\textdegree  poziomo.

Gogle zostaną wykorzystane do wyświetlania obrazu przechwyconego przez kamery.

-- tutaj można wstawić jakiś schemat oculusa --

\subsection{Logitech Webcam C910 Pro HD}

C910 Pro HD to kamera USB stworzona przez firmę Logitech. Umożliwia przechwytywanie obrazu o rozdzielczości 1920 pikseli na 1080 pikseli. Posiada wbudowany mikrofon, jednak nie będzie on wykorzystywany w naszej pracy.

Wydział MiNI zapewnił dwa egzemplarze tych kamer na rzecz niniejszej pracy. W związku z tym, że panoramiczny obraz, standardowo zapewniany przez kamery, jest daleki od natury ludzkiego oka ograniczymy się do obrazu 4:3. Zatem rozdzielczość obrazu w naszej pracy będzie wynosiła 1440 pikseli na 1080 pikseli na jedno oko.

Obudowa tej kamery jest dość duża, szczególnie w pozycji poziomej. Jest to problematyczne, ponieważ minimalna odległość między soczewkami, na jaką możemy umieścić kamery w tej pozycji, wynosi około 100 milimetrów. Jest to znacznie powyżej górnego ograniczenia odległości między ludzkimi źrenicami. W związku z tym konieczne będzie umieszczenie tych kamer w pozycji pionowej. Skutkuje to tym, że efektywna rozdzielczość przechwytywanego obrazu będzie wynosiła 1080 pikseli na 1440 pikseli.

\section{Odwzorowanie wizji stereoskopowej}

Uzyskanie efektu wizji trójwymiarowej podczas używania gogli z zaimplementowanym rozwiązaniem jest jednym z najważniejszych celów tego projektu. W tym podrozdziale znajduje się odniesienie do cech wizji stereoskopowej, opisanych w poprzednim rozdziale, pod kątem tego, jak wpływają na tworzone rozwiązanie w ramach niniejszej pracy.

\subsection {Parametryzacja wizji}

Wizję stereoskopową można opisać trzema parametrami:

\begin{itemize}
\item Kąt widzenia kamer
\item Odległość między obiektywami kamer
\item Kąt odchylenia osi widzenia
\end{itemize}

Pierwszy z tych parametrów - kąt widzenia kamer, jest zależny bezpośrednio od modelu kamery zastosowanego w projekcie. Jedyna możliwa jego regulacja polega na obcięciu brzegów obrazu, zmniejszając tym samym kąt widzenia. W niniejszej pracy istotnym jest, aby kąt widzenia kamer był zbliżony do kąta widzenia zapewnianego przez gogle Oculus Rift.

Dwa kolejne - odległość między kamerami i odchylenie osi widzenia, są istotne przy uzyskaniu efektu głębi podczas patrzenia przez gogle. Przy założeniu, że obserwujemy jeden punkt, istnieje korelacja między tymi parametrami pozwalająca uzyskać efekt głębi patrząc na przedmioty w okolicy obserwowanego punktu nawet w syttuacji, gdy odległość, bądź odchylenie odbiegają od idealnego ustawienia dla danego użytkownika. Przykładem będzie odchylenie osi widzenia do środka w przypadku, gdy odległości między obiektywami są większe, niż między oczami użytkownika. Jednakże efekt w ten sposób uzyskany jest tylko punktowy, a obiekty położone w innej odległości od punktu odniesienia nie będą widziane jako trójwymiarowe.

\subsection{Cechy ludzkiej wizji w projekcie}

W podrozdziale \textit{Wizja stereoskopowa człowieka} zostały przedstawione cechy ludzkiej wizji, które mają wpływ na przebieg niniejszej pracy. W tym podrozdziale omówione zostaną założenia, jakie zostały podjęte podczas realizacji projektu w celu spełnienia tych wymagań, oraz uzasadnienie w przypadku braku możliwości spełnienia danego wymagania.

\begin{description}

\item[Odległość między źrenicami] \hfill \\
W celu zapewnienia komfortowego korzystania z systemu jak największej grupie użytkowników konieczne jest opracowanie sposobu umieszczenia kamer zapewniającego:
\begin{itemize}
\item Umieszczenie kamer w stałej odległości od siebie podczas użytkowania systemu
\item Regulację odległości kamer od siebie
\end{itemize}

Kamery docelowo miały zostać umieszczone na goglach, ale przewidziana jest możliwość mocowania ich na czym innym, jednak z zachowaniem punktów wymienionych powyżej.

\item [Akomodacja oka] \hfill \\
Zjawisko akomodacji oka w niniejszej pracy jest nie do odwzorowania z wykorzystaniem dostępnego sprzętu. Wymaga ono śledzenia gałki ocznej w celu wyznaczenia punktu, który jest obserwowany przez użytkownika, a następnie wyostrzenie obrazu w danym punkcie. Gogle Oculus Rift nie mają obecnie takiej możliwości.

\item[Zbieżność oczu] \hfill \\
Zjawisko zbieżności oczu w niniejszej pracy również jest nie do odzworowania z takich samych powodów co zjawisko \textit{akomodacji oka}. Należałoby wyznaczyć punkt obserwowany przez użytkownika, a następnie skierować kamery w dany punkt. Wymagałoby to też stworzenia urządzenia, które poruszałoby kamerami w zależności od obserwowanego punktu. Wykracza to znacznie poza zakres tej pracy.

\end{description}

\chapter {Implementacja projektu}

W tym rozdziale przedstawione zostaną efekty pracy. Omówione szczegóły techniczne zaimplementowanego rozwiązania.

\section {Wybrane technologie i oprogramowanie}

\subsection {Języki i technologie }

\begin{description}
\item [Język programowania C\# ] \hfill \\
System został wykonany z wykorzystaniem języka programowania C\# oraz platformy programistycznej .NET Framework. Wybór tej technologii był podyktowany doświadczeniem zespołu w jej wykorzystaniu. Również wykorzystany silnik Unity3D wspiera .NET co sugerowało uniknięcie przyszlych problemów integracją modułów. W pracy .NET Framework został wykorzystany do napisania funkcjonalności pobierania obrazu z kamer i przetwarzania go, zanim trafi do modułu przekazywania obrazu do gogli.

\item [EmguCV] \hfill \\
Biblioteka opakowująca popularną bibliotekę do przetwarzania obrazów - OpenCV, w interfejs umożliwiający wykorzystanie jej w aplikacjach napisanych w języku C\#. Wykorzystana do implementacji modułów przechwytywania i przetwarzania obrazu. Przykładowe algorytmy przetwarzania obrazu również zostały napisane z wykorzystaniem tej biblioteki.

\item [Silnik Unity3D] \hfill \\
opis - piotrek

\item [Windows Presentation Foundation] \hfill \\
Windows Presentation Foundation (WPF) jest technologią służącą do tworzenia interfejsów użytkownika w aplikacjach desktopowych w środowisku .NET. W niniejszej pracy technologia ta została wykorzystana do napisania aplikacji konfiguracyjnej, która szerzej opisana będzie w dalszej części pracy.

\item [Windows Communication Foundation] \hfill \\
Windows Communication Foundation (WCF) jest technologią integrującą i unifikującą wszystkie technologie służące do komunikacji międzyprocesowej w aplikacjach opartych o .NET Framework. W niniejszej pracy WCF został wykorzystany do zaimplementowania komunikacji pomiędzy procesem głównym systemu, a aplikacją konfiguracyjną.
\end{description}

\subsection {Oprogramowanie}

\begin{description}
\item [Microsoft Visual Studio] \hfill \\
Zintegrowane środowisko programistyczne firmy Microsoft. Jest to najpopularniejsze IDE wykorzystywane w tworzeniu aplikacji opartych o .NET Framework.
\item [SoapUI] \hfill \\
Aplikacja służąca do testów serwisów webowych. Wykorzystana przez nas do testów połączenia pomiędzy procesem głównym, a aplikacją konfiguracyjną.
\end{description}

\section {Schemat systemu}

\subsection {Droga obrazu}

Planując wykonanie projektu droga, jaką musi przebyć obraz od przechwycenia go przez kamery do przekazania go do oka użytkownika, została podzielona na trzy warstwy. Są to:

\begin{description}
\item [Warstwa przechwytywania obrazu] \hfill \\ Jest odpowiedzialna za pobieranie obrazów z kamer i przygotowanie ich do dalszego przetwarzania - rotację obrazu w zależności od położenia kamery.
\item [Warstwa przetwarzania obrazu] \hfill \\ Wykonuje przetwarzanie obrazu według algorytmów zdefiniowanych przez użytkownika lub programistę.
\item [Warstwa przekazywania obrazu do gogli] \hfill \\ Odpowiada za konwersję obrazu do postaci przyjmowanej przez gogle VR.
\end{description}

-- też może schemat (chyba w prezentacji na seminarium był), opis słowny

\subsection{Architektura systemu}
TODO 

\subsection{Funkcjonalność przetwarzania obrazu}
TODO

\section {Opis modułów}

-- Tutaj wrzucimy szczegółowy techniczny opis modułów. Jako szczegółowy opis mam na myśli jego kluczowe funkcjonalności, jak wielowątkowe pobieranie obrazu itp.

\subsection {Moduł przechwytywania obrazu}

W tym module zaimplementowane są wszystkie operacje powiązane z kamerami. Kontrakt tego modułu przedstawia się następująco:

\lstset{style=sharpc}
\begin{lstlisting}
    public interface IViewCalibrator
    {
        void RotateImage(CaptureSide captureSide, RotateSide rotateSide);
    }

    public interface ICaptureManager
    {
        void SetCapture(CaptureSide captureSide, int cameraIndex);
        CaptureDetails GetCaptureDetails();
    }

    public interface IViewProvider : IViewCalibrator, ICaptureManager
    {      
        ViewDataBitmap GetCurrentViewAsBitmaps();               
        ViewDataImage GetCurrentView();
        void UpdateFrames();
    }
\end{lstlisting}

Interfejs \textit{IViewCalibrator} jest odpowiedzialny za operacje związane z kalibracją obrazu. W tym projekcie jedyną taką operacją jest obrót obrazu w celu zrównoważenia ewentualnego obrotu kamery (metoda \textit{RotateImage}).
Interfejs \textit{ICaptureManager} odpowiada za wszystko co jest związane z przechwytywaniem obrazu. Metoda \textit{SetCapture} ustawia dany kanał na kamerę o danym indeksie w systemie, a metoda \textit{GetCaptureDetails} pobiera informacje o przechwytywanym obrazie, na przykład rozdzielczość obrazu, jego rotację czy indeks kamery, z której pochodzi.
Interfejs \textit{IViewProvider} jest głównym interfejsem, który jest implementowany przez moduł. Obejmuje sobą funkcjonalności interfejsów poprzednio wymienionych oraz udostępnia metody bezpośrednio związane z pobieraniem obrazu. Metoda \textit{UpdateFrames} wysyła sygnał do wątków pobierających obraz, aby pobrały kolejną klatkę. Pozostałe dwie metody - \textit{GetCurrentView} i \textit{GetCurrentViewAsBitmaps} pobierają obecne klatki w różnych formatach. Pierwsza z nich, wykorzystywana przez proces główny, pobiera obraz jako klasy Image biblioteki EmguCV. Druga jest wykorzystywana przez aplikację konfiguracyjną, a obraz pobrany z jej wykorzystaniem jest postaci klas System.Drawing.Bitmap.

Z powyższego opisu wynika, że najważniejsze operacje modułu to:
\begin{itemize}
\item Wybór kamery dla danego kanału.
\item Pobranie obrazu z wybranych kamer dla obu kanałów
\item Obrót obrazu w celu zrównoważenia ewentualnego obrotu kamery.
\end{itemize}

Poniżej są przedstawione zasady działania każdej z tych operacji:

\begin{description}
\item [Wybór kamery dla danego kanału] \hfill \\
Dla każdego kanału w module jest pole, w którym jest przechowywany obiekt odpowiedzialny za komunikację z daną kamerą. Do tych pól odwołują się pozostałe funkcjonalności, pobierające obraz. W przypadku zmiany kamery w danym kanale, wartość odpowiedniego pola jest podmieniana na obiekt odpowiadający kamerze docelowej. Zmiana ta jest transparentna dla pozostałych funkcjonalności.
\item [Pobranie obrazu z wybranych kamer dla obu kanałów] \hfill \\
Operacja jest zrealizowana wielowątkowo. Każdy z dwóch kanałów ma swój wątek, czeka na sygnał pobrania klatki, pobiera klatkę wykonując jednocześnie jej obrót, wysyła sygnał informujący o zakończeniu pobierania klatki. Operacje te wykonują się w pętli dopóki wątek nie zostanie przerwany.
\item [Obrót obrazu w celu zrównoważenia ewentualnego obrotu kamery] \hfill \\
Moduł przechowuje informację o ilości obrotów o kąt prosty obrazu dla danego kanału. Zwiększenie (zmniejszenie) kąta obrotu polega na inkrementacji (dekrementacji) tej wartości poprzez wywołanie odpowiedniej metody. Na podstawie tej wartości przy pobraniu każdej klatki obliczany jest kąt, o który dany obraz ma być obrócony.
\end{description}

Funkcjonalności modułu są współdzielone przez aplikację główną oraz konfiguracyjną. Ta część kontraktu, która jest wykorzystywana przez aplikację konfiguracyjną, jest wydzielona jako kontrakt serwisu udostępnionego tej aplikacji, zgodnie ze wzorcem projektowym \textit{fasada}. Dzięki temu ukryte zostały zbędne funkcjonalności modułu z punktu widzenia aplikacji konfiguracyjnej upraszczając jego interfejs.


Podczas testów modułu wystąpiły sytuacje, gdzie aplikacja zawieszała się na wywołaniu metody z biblioteki EmguCV pobierającej kolejną klatkę z kamery. Występowało to w sytuacji, kiedy obiekt odpowiedzialny za pobieranie obrazu był ustawiony na kamerę, która nie istniała w systemie (jej indeksowi nie odpowiadało żadne urządzenie). Moduł zawiera rozwiązanie zapobiegające tego typu sytuacjom. Przy każdym pobraniu klatki do odpowiedniego pola jest zapisany czas jej pobrania. Podczas tworzenia instancji modułu tworzony i uruchamiany jest wątek, który co jakiś czas sprawdza czas od odstatniego pobrania klatki dla kanału. Jeżeli jest on większy od ustalonego dopuszczalnego maksimum, to wątek tego kanału jest przerywany, a w jego miejsce tworzony nowy. 
W zaimplementowanym rozwiązaniu maksymalna dopuszczalna przerwa między klatkami i czas co jaki jest ona sprawdzana wynosi 10 sekund.

\subsection {Moduł przetwarzania obrazu}

Moduł ten odpowiedzialny jest za realizację warstwy posredniej w naszym systemie, czyli przetwarzania obrazów po przechwyceniu ich z obu kamer, ale przed przekazaniem do gogli.  Do realizacji przekształceń na obrazie zostały wykorzystane specjalne obiekty wykonujące pojedyńcze przekształcenia. Obiekty znajdują sie na liscie, z której są wywoływane w kolejnosci występowania na niej w celu wykonania przekształcenia na podanym obrazie. Programista korzystający z naszej biblioteki może dodawać własne implementacje klas przetwarzających obraz i dodawać ich instancje do tej listy, przez co możliwa jest pełna kontrola procesu przetwarzania obrazu odbywającego się w tej częsci systemu. 

Kluczowym elementem modułu jest sposób włączenia warstwy przetwarzania obrazu w cały proces przekazywania obrazu od kamery do gogli. Dodatkowo stworzony został kontrakt definiujący metody, które muszą posiadać klasy zajmujące się pojedyńczymi przekształceniami obrazu.

\begin{description}

\item [Dodanie funkcjonalnosci przetwarzania obrazu] \hfill \\
Klasą bazową odpowiedzialną za pobieranie obrazu jest \textit{ViewProvider}, implementująca interfejs \textit{IViewProvider}  który został wykorzystanu w częsciach systemu związanych z tworzeniem serwisu do komunikacji się z aplikacją konfiguracyjną oraz przy przekazywaniu obrazu do gogli. W celu zachowania łatwej wymienialnosci kodu, wykorzystany został fakt, że warstwa przetwarzania obrazu jest rozszerzeniem warstwy pobierania obrazu o zastosowanie odpowiednich przekształceń na obrazie pobranym z kamery. Dlatego też zastosowalimy wzorzec projektowy \textit{Dekorator} i stworzylismy nowa klase \textit{ProcessedViewProvider} implementujaca również interfejs \textit{IViewProvider}, będącą rozszerzeniem klasy \textit{ViewProvider}


\lstset{style=sharpc}
\begin{lstlisting}
    public class ProcessedViewProvider : IViewProvider
    {
        private readonly IViewProvider _originViewProvider;
        private readonly List<IImageProcessor> _imageProcessors;

        public ProcessedViewProvider(IViewProvider originViewProvider,
			 List<IImageProcessor> imageProcessors = null);

        public void RotateImage(CaptureSide captureSide, RotateSide rotateSide);
        public void SetCapture(CaptureSide captureSide, int cameraIndex);
        public CaptureDetails GetCaptureDetails()
        public ViewDataBitmap GetCurrentViewAsBitmaps();
        public ViewDataImage GetCurrentView();
        public void UpdateFrames();
    }
\end{lstlisting}

W konstruktorze jako argument przyjmowany jest obiekt implementujący interfejs \textit{IViewProvider}. Wzorzec adapter realizowany jest poprzez wywoływanie w każdej metodzie metody o tej samej nazwie z bazowego obiektu przechowywanego w polu \textit{\textunderscore originViewProvider} oraz ewentualnym wykonaniu dodatkowych funkcji przed końcem metody. 

Kolejnym argumentem przekazywanym w konstruktorze jest lista obiektów implementujących interfejs \textit{IImageProcessor}. W metodach \textit{GetCurrentView} oraz \textit{GetCurrentViewAsBitmaps} po pobraniu obrazów z pierwotnego obiektu klasy \textit{ViewProvider} wykonanywana jest seria przekształceń za pomocą obiektów znajdujących się na wspomnianej liscie. W kolejnosci ich występowania, wywoływana jest metoda \textit{Process} której jako argument przekazywana jest referencja do obiektu przechowujacego obraz. W ten sposób obrazy zwracane w tych metodach przechodzą przez warstwę przetwarzania obrazów.


\item [Interfejs łączący implementacje przekształceń obrazu] \hfill \\
\lstset{style=sharpc}
\begin{lstlisting}
    public interface IImageProcessor
    {
        void Process(ref Image<Bgr, byte> image);
        string Name { get; }
        bool Active { get; set; }
    }
\end{lstlisting}

Metoda \textit{Process} jako argument przyjmuje referencje do obrazu reprezentowanego za pomocą typu Image<Bgr, byte> pochodzącego z biblioteki EmguCV. Zdecydowalimy się na takie rozwiązanie ponieważ operując na tym typie można wykorzystać różne metody pochodzące z biblioteki, które pozwalają na szybszą konwersje obrazu przeprowadzając częsć operacji przy wykorzystaniu karty graficznej oraz w celu uniknięcia dodatkowych konwersji na typy podstawowe języka C\#(np. tablica bajtów). Metoda nie zwraca żadnej wartosci lecz operuje na przekazanym obrazie stosując dla niego odpowiednie przekształcenia.  \\
Dodatkowo interfejs wymaga posiadania przez klasy następujących własciwosci:
\begin{itemize}
\item  \textit{Name}, typu tekstowego - zawierająca nazwę przekształcenia.
\item \textit{Active}, typu logicznego -  decydująca o tym czy przekształcenie będzie wykonane czy pominięte.
\end{itemize}

\end{description}

\subsection {Moduł przekazywania obrazu do gogli}

W tym module znajdują sie wszystkie operacje związane z przekazywaniem przechwyconego obrazu z kamer do gogli. Ponieważ poprzednie moduły zostały zaimplementowane przy użyciu języka C\# oraz w technologii .NET, do przekazywania obrazu do gogli OculusRift skorzystalismy z silnika Unity3d umożliwiającego importowanie bibliotek stworzonych w tych technologiach.

Unity3d jest jedynie silnikiem grafiki 3D i nie współpracuje bezposrednio z urządzeniami VR. Jednak producenci gogli zapewniają dedykowane biblioteki dla swoich urządzeń działające w tym silniku, pozwalające na dodanie obsługi wirtualnej rzeczywistosci oraz sterowanie przekazywanym obrazem dla każdego oka osobno. Wykorzystany został pakiet \textit{Oculus Utitites for Unity} udostępniany przez producenta gogli OculusRift (https://developer.oculus.com/downloads/unity/), który w samym edytorze silnika Unity nosi nazwę \textit{OVR plugin}.  \hfill \\

Wykorzystując funkcjonalnosci tego pakietu, stworzona została scena  w silniku Unity zawierająca dwie kamery i dwie tekstury. Kamery posiadają właciwosc o nazwie \textit{TargetEye} która pozwala decydowac czy obraz w polu widzenia kamery trafia do lewego lub prawego ekranu (oka) w urządzeniu OculusRift. Na kazdej z tekstur rowniez wyswietlany jest obraz tylko dla jednego oka. Tekstury renderowane są jedynie w całym polu widzenia kamer, zamiast bezposrednio jako obiekt standardowy obiekt sceny. Dzięki temu podczas obrotów kamery po wykryciu ruchu urządzenia VR, tekstura nie musi być poddawana żadnym przekształceniom zwiąnych z grafiką 3D  co wpływa na wydajnoć systemu. W celu wyswietlenia obrazu na teksturze potrzebna jest jego odpowiednia konwersja, dlatego ważną częcią modułu oprócz samego wyswietlania obrazu na goglach jest również konwersja obrazu z modułu przetwarzania obrazu do formy umożliwiającej wyswietlenie go na scenie w silniku Unity3D.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{unityscene}
\caption[Scena Unity]{Scena w programie Unity Editor}
\end{figure}

W module zostały przewidziane następujące interfejsy, zapewniające metody do realizacji kluczowych jego elementów.

\lstset{style=sharpc}
\begin{lstlisting}
    public interface IStereoVidTransmitter
    {
        StereoView GetStereoView();
    }

    public interface ITextureConverter
    {
        Texture FromBitmap(Bitmap source);
        Texture FromImage(Image<Bgr, byte> source);
        byte[] DataFromImage(Image<Bgr, byte> image);
        void LoadFromImage(Image<Bgr, byte> sourceImage, 
		Texture targetTexture);
    }

\end{lstlisting}

Interfejs \textit{IStereoVidTransmitter} jest odpowiedzialny za dostarczenie obrazów w odpowiednim formacie, gotowych do wywietlenia na scenie Unity3d. Metoda \textit{GetStereoView} zwraca obiekt zawierający dwie tekstury, dla lewego i prawego oka. 
Interfejs \textit{ITextureConverter} odpowiada za wszystkie operacje potrzebne przy konwersji obrazu do tekstury. W naszym systemie wykorzystywany jest przez klasy implementujące interfejs \textit{IStereoVidTransmitter}. Metoda \textit{FromBitmap} zwraca nowy obiekt tekstury utworzony na podstawie przekazanego obrazu za pomocą typu \textit{System.Drawing.Bitmap}.\textit{FromImage} działa analogicznie do poprzedniej, jednak jako argument przyjmuje obraz w postaci typu pochodzącego z biblioteki EmguCV.
 Metoda \textit{DataFromImage} zwraca tablice typu \textit{byte} z przekazanego do niej obrazu, która potrzebna jest do konwersji obrazu na teksturę w silniku Unity. Natomiast metoda \textit{LoadFromImage}  ładuje obraz z przekaznego obrazu do już istniejącej tekstury, co wykorzystywane jest w celu poprawnienia wydajnosci i uniknięcia niepotrzebnego tworzenia tesktur za każdym razem od nowa przy każdej konwersji obrazu.

Z powyższego opisu wynika, że najważniejsze operacje modułu to:
\begin{itemize}
\item Przekazanie i wyswietlanie obrazu
\item Konwersja obrazu
\end{itemize}

Poniżej są przedstawione zasady działania każdej z tych operacji:

\begin{description}
\item [Przekazanie i wyswietlanie obrazu] \hfill \\
Po uruchomieniu aplikacji Unity, wraz ze stworzeniem sceny ładowany jest skrypt działający przez cały czas życia aplikacji. W skrypcie tym inicjowane  są klasy odpowiadające za pobieranie obrazu z kamer i poddanie go odpowiednim przekształceniom. Za każdym razem, gdy silnik Unity próbuje odswiezyc obraz, pobierane są dwie klatki obrazu ze wspomnianych klas - dla lewego i prawego oka. Następnie poddawane sa one konwersji do formatu tekstury silnika Unity.  Po przygotowaniu tekstur zawierających ostatnio pobrane obrazy z kamery, tekstury na scenie są aktualizowane i do kamer, a później do gogli trafia odwieżony obraz.
\item [Konwersja obrazu] \hfill \\
Konwersja nie odbywa się w pełni wielowątkowo, ponieważ operacje na obiektach silnika Unity mogą być realizowane tylko w głównym wątku (dzieje się tak ponieważ Unity automatycznie wykonuje częsć operacji na karcie graficznej i synchronizacja takich operacji odbywających się w w wielu wątkach byłaby bardzo problematyczna). Jednak w celu maksymalnego poprawienia wydajnosci, w osobnych wątkach z obrazy konwertowane są do danych w postaci tablicy bajtów, a dopiero sam proces ładowania danych z tych tablic do tekstury będącej obiektem silnika Unity3d następuje synchronicznie w głównym wątku. 
\end{description}

\subsection {Aplikacja konfiguracyjna}
TODO

\subsection {Serwis komunikacji międzyprocesowej}
TODO

\chapter {Rezultat pracy}

Ten rozdział jest poświęcony zbadaniu, czy wdrożone rozwiązanie jest tym, co zostało wyspecyfikowane w rozdziale \textit{Cel i założenia projektu}. Przedstawione zostaną wyniki testów użytkowych i wydajnościowych.
\section {Testy rozwiązania}
TODO

\subsection {Testy twarde, liczbowe, itp.}
TODO wyniki testów opóźnień, klatek na sekundę

\subsection {Testy subiektywne}
TODO Czytanie książki. Łapanie przedmiotu, itp kreatywne pomysły

\section {Wnioski}
TODO 

\subsection {Co wyszło dobrze}
TODO

\subsection {Co nie wyszło dobrze}
TODO

\chapter {Przyszłość rozwiązania}
TODO

% -------------------- 6. Bibliografia -----------------------
% Bibliografia leksykograficznie wg nazwisk autorów

\begin{thebibliography}{20}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}

\bibitem[1]{Ktos} A. Aaaaa, \emph{Tytuł}, Wydawnictwo, rok, strona-strona.
\bibitem[2]{Innyktos} J. Bobkowski, S. Dobkowski, \emph{Blebleble}, Magazyn nr, rok, strony.
\bibitem[3]{B} C. Brink, \emph{Power structures}, Algebra Universalis 30(2), 1993, 177-216.
\bibitem[4]{H} F. Burris, H. P. Sankappanavar, \emph{A Course of Universal Algebra}, Springer-Verlag, New York, 1981.
\end{thebibliography}
\thispagestyle{empty}


% --- 7. Wykaz symboli i skrótów - jeśli nie ma, zakomentować
\chapter*{Wykaz symboli i skrótów}

\begin{tabular}{cl}
nzw. & nadzwyczajny \\
* & operator gwiazdka \\
HUD & head-up display \\
FPS & frames per second \\
IDE & Integrated Development Environment \\
HTTP & Hypertext Transfer Protocol \\
$\widetilde{}$ & tylda
\end{tabular}
\thispagestyle{empty}


% ----- 8. Spis rysunków - jeśli nie ma, zakomentować --------
\listoffigures
\thispagestyle{empty}


% ------------ 9. Spis tabel - jak wyżej ------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}



% 10. Spis załączników - jak nie ma załączników, to zakomentować lub usunąć

\chapter*{Spis załączników}
\begin{enumerate}[itemsep = 0pt]
\item Załącznik 1
\item Załącznik 2
\end{enumerate}
\thispagestyle{empty}

% --------------------- 11. Załączniki ---------------------
% to jest po to, żeby było wiadomo, że załączniki znajdują się na końcu pracy

\newpage
\pagestyle{empty} 
Załącznik 1, załącznik 2
\end{document}
